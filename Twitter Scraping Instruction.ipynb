{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction to Twitter Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Twitter Scraping with Python Library GetoldTweets3\n",
    "#### Some Reminders:\n",
    "#### 1) Use \"pip install GetOldTweets3\" in Comment Prompt to install the package\n",
    "#### 2) Meaningful keywords related to power outages: power outage, power outages, #poweroutage, #poweroutages, power is out, power is down, no electricity, #noelectricity, outage, outages, #outages    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries \n",
    "import os\n",
    "import pandas as pd \n",
    "import GetOldTweets3 as got "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the working directory for saving tweet files \n",
    "os.chdir(\"E:/Alliant/multipleRunning/no electricity/2018\")\n",
    "## Set parameters\n",
    "from_date = \"2020-04-24\"\n",
    "to_date = \"2020-04-25\"\n",
    "maxTweets = 14000\n",
    "searchQuery = \"#poweroutage\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded #poweroutage2020-04-24to2020-04-25\n"
     ]
    }
   ],
   "source": [
    "### Download tweets \n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(searchQuery)\\\n",
    "                                            .setSince(from_date)\\\n",
    "                                            .setUntil(to_date)\\\n",
    "                                            .setMaxTweets(maxTweets) \n",
    "\n",
    "tweets = got.manager.TweetManager.getTweets(tweetCriteria) \n",
    "\n",
    "## Retrieve information \n",
    "author_id = [] \n",
    "date = []  \n",
    "text = [] \n",
    "username = [] \n",
    "\n",
    "for tweet in tweets: \n",
    "\n",
    "    id = str(tweet.author_id) \n",
    "    author_id.append(id) \n",
    "    \n",
    "    dt = str(tweet.date) \n",
    "    date.append(dt) \n",
    "     \n",
    "    txt = str(tweet.text) \n",
    "    text.append(txt) \n",
    "\n",
    "    user = str(tweet.username) \n",
    "    username.append(user) \n",
    "     \n",
    "## Aggregate into file \n",
    "author_id = pd.Series(author_id) \n",
    "date = pd.Series(date)  \n",
    "text = pd.Series(text) \n",
    "username = pd.Series(username) \n",
    "\n",
    "tweet_data = pd.concat([author_id, username, date, text], axis = 1) \n",
    "tweet_data.columns = ['author id', 'username', 'date', 'text'] \n",
    "tweet_data.to_csv(\"{}_{}_to_{}.csv\".format(searchQuery,from_date,to_date))\n",
    "\n",
    "print(\"Downloaded \" + str(searchQuery) + str(from_date)+ \"to\" + str(to_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieve user location with username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some reminders:\n",
    "#### 1) Use \"pip install tweepy\" in Comment Prompt to install the package\n",
    "#### 2) This method requires Twitter API keys. To obtain a Twitter Developer account and API keys, please go to: https://developer.twitter.com/en\n",
    "#### 3) Not all users list a profile location: 1/3 of Tweets have user profile location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries \n",
    "import tweepy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in credentials \n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "auth.set_access_token(access_token, access_token_secret) \n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True) \n",
    "\n",
    "if (not api): \n",
    "    print (\"Problem Connecting to API\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1/22 tweets\n",
      "Downloaded 2/22 tweets\n",
      "Downloaded 3/22 tweets\n",
      "Downloaded 4/22 tweets\n",
      "Downloaded 5/22 tweets\n",
      "Downloaded 6/22 tweets\n",
      "Downloaded 7/22 tweets\n",
      "Downloaded 8/22 tweets\n",
      "Downloaded 9/22 tweets\n",
      "Downloaded 10/22 tweets\n",
      "Downloaded 11/22 tweets\n",
      "Downloaded 12/22 tweets\n",
      "Downloaded 13/22 tweets\n",
      "Downloaded 14/22 tweets\n",
      "Downloaded 15/22 tweets\n",
      "Downloaded 16/22 tweets\n",
      "Downloaded 17/22 tweets\n",
      "Downloaded 18/22 tweets\n",
      "Downloaded 19/22 tweets\n",
      "Downloaded 20/22 tweets\n",
      "Downloaded 21/22 tweets\n",
      "Downloaded 22/22 tweets\n",
      "Downloaded #poweroutage2020-04-24to2020-04-25 with location\n"
     ]
    }
   ],
   "source": [
    "### Retrieve user location\n",
    "tweetCount=0 \n",
    "user_loc = []\n",
    "for i in range(len(author_id)): \n",
    "    \n",
    "    user = str(username[i]) \n",
    " \n",
    "    try: \n",
    "        user_json = api.get_user(str(\"@\" + user)) \n",
    "        userloc = user_json.location \n",
    "        if userloc: \n",
    "            user_loc.append(userloc) \n",
    "        else: \n",
    "            user_loc.append(\"N/A\")\n",
    "            \n",
    "    except tweepy.TweepError as e: \n",
    "        user_loc.append(\"N/A\") \n",
    "        print(\"some error : \" + str(e))         \n",
    "\n",
    "     \n",
    "    tweetCount += 1 \n",
    "    print(\"Downloaded {}/{} tweets\".format(tweetCount, len(author_id))) \n",
    "\n",
    "user_loc = pd.Series(user_loc)    \n",
    "tweet_data_with_loc = pd.concat([author_id, username, user_loc, date, text], axis = 1) \n",
    "tweet_data_with_loc.columns = ['author id', 'username', 'user location', 'date', 'text'] \n",
    "tweet_data_with_loc.to_csv(\"{}_{}_to_{}_with_loc.csv\".format(searchQuery,from_date,to_date))\n",
    "\n",
    "print(\"Downloaded \" + str(searchQuery) + str(from_date)+ \"to\" + str(to_date) + \" with location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieve and identify different states of the US for each user_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) User locations are manually entered, which is not always clean data.\n",
    "#### 2) Some user locations that have capital “WI” or “IA” in the string get included in our filtering such as “NIGERIA”.\n",
    "#### 3) A CSV file containing all the state names and abbreviation will be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read a CSV file containing all the state names from the Github\n",
    "stateList = pd.read_csv('https://raw.githubusercontent.com/dengziqian/msba_AlliantEnergy/master/stateList.csv')\n",
    "stateList_lowcase = pd.DataFrame({\"State Lowercase\":stateList[\"State\"].str.lower(), \"Abbreviation\":stateList[\"Abbreviation\"]})\n",
    "new_stateList = stateList.join(stateList_lowcase.set_index(\"Abbreviation\"), on=\"Abbreviation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded #poweroutage2020-04-24to2020-04-25 with locationwith State\n",
      "Downloaded #poweroutage2020-04-24to2020-04-25 with locationin WI & IA\n"
     ]
    }
   ],
   "source": [
    "#Create an empty dataframe to save state information later\n",
    "authorid_State=pd.DataFrame({\"username\":[], \"state\":[]})\n",
    "ae = tweet_data_with_loc\n",
    "for i in range(len(new_stateList)): \n",
    "    \n",
    "    #In each tweet, search for each state name under \"user location\" column and return that tweet to a dataframe\n",
    "    tweets_with_state_name = ae[ae[\"user location\"].str.contains(str(new_stateList.iloc[i][0]), na=False)]\n",
    "    \n",
    "    #In each tweet, search for each state code under \"user location\" column and return that tweet to a dataframe\n",
    "    tweets_with_state_code = ae[ae[\"user location\"].str.contains(str(new_stateList.iloc[i][1]), na=False)]\n",
    "    \n",
    "    #In each tweet, search for each lower case state name under \"user location\" column and return that tweet to a dataframe\n",
    "    tweets_with_state_name_lowercase = ae[ae[\"user location\"].str.contains(str(new_stateList.iloc[i][2]), na=False)]\n",
    "    \n",
    "    #Combine there dataframes into one\n",
    "    tweets_with_state = pd.concat([tweets_with_state_name, tweets_with_state_code, tweets_with_state_name_lowercase])\n",
    "    \n",
    "    #Create another dataframe to record the state code for the users\n",
    "    state = pd.DataFrame({\"username\": tweets_with_state[\"username\"], \"state\":str(stateList.iloc[i][1])})\n",
    "    \n",
    "    #Store the state information in an empty dataframe and keep adding information in it\n",
    "    authorid_State = pd.concat([authorid_State, state])\n",
    "\n",
    "#Since one user may have two states identified, I here keep the first state and remove the rest\n",
    "authorid_State.drop_duplicates(subset = \"username\", keep='first', inplace=True)\n",
    "\n",
    "#Join the original tweets file with authorid_State file on username\n",
    "ae_join = ae.join(authorid_State.set_index('username'), on='username')\n",
    "ae_join.to_csv(\"{}_{}_to_{}_with_loc_with_State.csv\".format(searchQuery,from_date,to_date), index=False)\n",
    "print(\"Downloaded \" + str(searchQuery) + str(from_date)+ \" to \" + str(to_date) + \" with_location_\" + \"with_State\")\n",
    "\n",
    "#Extract tweets from WI and IA\n",
    "ae_wi = ae_join[ae_join[\"state\"]==\"WI\"]\n",
    "ae_ia = ae_join[ae_join[\"state\"]==\"IA\"]\n",
    "ae_wi_ia = pd.concat([ae_wi,ae_ia])\n",
    "\n",
    "ae_wi_ia.to_csv(\"{}_{}_to_{}_with_loc_in_WI&IA.csv\".format(searchQuery,from_date,to_date))\n",
    "print(\"Downloaded \" + str(searchQuery) + str(from_date)+ \" to \" + str(to_date) + \" with_location_\" + \"in_WI & IA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
